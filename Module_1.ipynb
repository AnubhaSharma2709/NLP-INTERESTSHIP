{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installing NLTK** :\n",
        "\n",
        "NLTK, short for Natural Language ToolKit, is a library written in Python for symbolic and statistical Natural Language Processing."
      ],
      "metadata": {
        "id": "lowLmcbEloW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC0N5SRrJ4UC",
        "outputId": "1e9d534d-7616-4814-9b11-5be18dfb80cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punkt tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences.**"
      ],
      "metadata": {
        "id": "3Ah_-s_MnIzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5Pc8eJekV4K",
        "outputId": "e15866e4-0166-415e-87da-49eb9bd6cb09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK contains a module called tokenize() which further classifies into two sub-categories: Sentence Tokenization and Word Tokenization.**"
      ],
      "metadata": {
        "id": "d1YDRjYdxw7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.\n",
        "\n",
        "1.   Text into sentences tokenization\n",
        "2.   Sentences into words tokenization\n",
        "\n"
      ],
      "metadata": {
        "id": "lFE7PBDnnyd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Tokenization - We use the sent_tokenize() method to split a document or paragraph into sentences**"
      ],
      "metadata": {
        "id": "8ZfBOVBCo421"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to NLP Interestship. We are learning NLP basics\"\n",
        "sent_tokenize(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkj9zwUaoELA",
        "outputId": "b0e8a7ce-025d-4e6b-913f-ce74add60526"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to NLP Interestship.',\n",
              " 'We are learning NLP basics']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello everyone Welcome to NLP Interestship We are learning NLP basics\"\n",
        "text1.split(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBjNK5bu-8_A",
        "outputId": "c6d10d4a-5bab-4ac5-c0ba-d5eea647e1f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello,',\n",
              " 'everyone,',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'NLP',\n",
              " 'Interestship,',\n",
              " 'We',\n",
              " 'are',\n",
              " 'learning',\n",
              " 'NLP',\n",
              " 'basics']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenization - We use the word_tokenize() method to split a sentence into tokens or words**"
      ],
      "metadata": {
        "id": "fTky3ovRpCDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to Day 1 of Interestship.\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "id": "3KYYEzPnR9o2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ba1fb2-7224-414a-d769-920445ebb1f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'everyone',\n",
              " '.',\n",
              " 'Welcome',\n",
              " 'to',\n",
              " 'Day',\n",
              " '1',\n",
              " 'of',\n",
              " 'Interestship',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how NLTK is considering punctuation as a token? Hence for future tasks, we need to remove the punctuations from the initial list."
      ],
      "metadata": {
        "id": "TMT5O4VmxSIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming**"
      ],
      "metadata": {
        "id": "9xrwkLj6zYJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing modules\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer() #creating an instance of the class\n",
        "\n",
        "# creating a list of some words to be stemmed\n",
        "words = ['run','ran','running','studies']\n",
        "\n",
        "for word in words:\n",
        "    print(word, \" : \", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G11MofdzzaM5",
        "outputId": "ab1d13b6-afc8-434d-eec7-9e45359f4aea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run  :  run\n",
            "ran  :  ran\n",
            "running  :  run\n",
            "studies  :  studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization**"
      ],
      "metadata": {
        "id": "sSCPsGw_z6Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create WordNetLemmatizer object\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "# single word lemmatization examples\n",
        "words = ['run','ran','running','studies']\n",
        "for word in words:\n",
        "    print(word + \" ---> \" + lemma.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_jNpK-jzn3b",
        "outputId": "55ebf5ee-0df9-4be5-ad7d-c7132af08568"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run ---> run\n",
            "ran ---> ran\n",
            "running ---> running\n",
            "studies ---> study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations, You completed Module 1.**"
      ],
      "metadata": {
        "id": "EaTX3BI60la2"
      }
    }
  ]
}

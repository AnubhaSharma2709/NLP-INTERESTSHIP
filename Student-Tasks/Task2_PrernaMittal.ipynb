{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression\n",
        "\n",
        "* It is a classification algorithm\n",
        "* It works on Sigmoid function ie. values above a certain threshold become 1 and values below it, become 0\n",
        "* It is used when the data is linearly separable ie. data can be separated using a line\n",
        "\n",
        "Pros:\n",
        "* less prone to overfitting compared to more complex algorithms.\n",
        "* It can handle scaled features and does not require normalization or standardization of the input data.\n",
        "\n",
        "Cons:\n",
        "*  It may not perform well when the relationship is non-linear.\n",
        "* It works best with numerical features and may require encoding or transformation of categorical variables.\n",
        "\n",
        "Example:\n",
        "\n",
        "There's a dataset with two features: \"Age\" (ranging from 0 to 100) and \"Income\" (ranging from 10,000 to 1,000,000). Logistic regression can handle these features as they are, without requiring any scaling. It can estimate the coefficients and make predictions based on the original scale of the features."
      ],
      "metadata": {
        "id": "WS1BSc3pqzP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "input_feature_x=np.array([4,12,16,20,24,36,40]).reshape((-1,1)) #maps to output_y values\n",
        "output_y=np.array([0,0,0,1,0,1,1])\n",
        "logistic_model=LogisticRegression()\n",
        "logistic_model.fit(input_feature_x, output_y) #independent features, target value\n",
        "prediction_array=np.array([13,16]).reshape(-1,1) #predict output for 13, 16 input values\n",
        "logistic_model.predict(prediction_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ileE7w32rSBs",
        "outputId": "06bd5223-3042-49a8-c98b-11a202e8d3b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes\n",
        "* It is based on Bayes’s theorem.\n",
        "* It assumes that the features are independent\n",
        "\n",
        "Pros:\n",
        "\n",
        "* This algorithm works very fast.\n",
        "* It can also be used to solve multi-class prediction problems as it’s quite useful with them.\n",
        "* This classifier performs better than other models with less training data if the assumption of independence of features holds.\n",
        "* It has few hyperparameters, making them easy to implement and tune.\n",
        "\n",
        "Cons:\n",
        "\n",
        "* It assumes that all the features are independent which means anyone can hardly find a set of independent features. It is unrealistic for real-world datasets\n",
        "\n",
        "Example:\n",
        "\n",
        "In a sentiment analysis task where the goal is to classify movie reviews as positive or negative, Naive Bayes algorithm can analyze the occurrences of different words in the reviews and estimate the likelihood of a review being positive or negative based on the observed word frequencies in each class."
      ],
      "metadata": {
        "id": "o5-utXsVq5w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "input_x=np.array([[-2,-1],[-3,-1],[-4,-3],[1,1],[3,2],[6,4]])\n",
        "output_y=np.array([1,1,1,2,2,2])\n",
        "nb_classifier=GaussianNB()\n",
        "nb_classifier.fit(input_x,output_y)\n",
        "prediction_array=[[8,5]]\n",
        "nb_classifier.predict(prediction_array) #classifier predicts the input to belong to class label 2."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbru8gGUSpN",
        "outputId": "512966eb-60db-4b97-a2ec-45a25568fe1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "902wZUCoqKyG",
        "outputId": "ea0163cd-62f9-4c9f-dc88-99c51e1bf41c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 2 1 1 0 0 2 1 1 1 2 0 1 0 2 1 1 2 2 1 0 1 2 1 2 2 0 1 2 1 2 1 2 2 1\n",
            " 2]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "X, y = load_iris(return_X_y=True) #returns the feature matrix X and the target vector y\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=142) #used 25% of data for testing\n",
        "nb_classifier = GaussianNB() #classifier follows Gaussian (normal) distribution\n",
        "nb_classifier.fit(X_train, y_train) #train the classifier using the training data\n",
        "prediction_results = nb_classifier.predict(X_test)  #predictions on the test data\n",
        "print(prediction_results) #prints the array of predicted class labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performance Metrics\n",
        "\n",
        "* Confusion Matrix\n",
        "  * TP, TN, FP(Type-1 error), FN(Type-2 error)\n",
        "  * It is particularly useful when dealing with imbalanced classes\n",
        "  * It can be visually represented as a heatmap\n",
        "* Precision\n",
        "  * focuses on Type-1 error only\n",
        "  * range is 0 to 1\n",
        "  * tells the correctness of positive predictions\n",
        "  * TP/(TP+FP)\n",
        "  * high precision score signifies FP is low and model is performing good\n",
        "  * low precision score can be due to imbalanced dataset or hyperparameters of the model are not tuned properly.\n",
        "* Recall\n",
        "  * focuses on Type-2 error only\n",
        "  * TP/(TP+FN)\n",
        "  * high recall means low FN\n",
        "  * recall will be low when (same as precision)\n",
        "* Accuracy\n",
        "  * correct predictions/total no. of predictions\n",
        "  * (TP+TN)/(TP+FP+TN+FN)\n",
        "* F1 Score\n",
        "  * combines both precision and recall (harmonic mean of both)\n",
        "  * 2 * (precision * recall)/(precision+recall)\n",
        "  * high F1 score means precision and recall is well-balanced\n",
        "  * cannot comment in case of low F1 score"
      ],
      "metadata": {
        "id": "4U2Ja0Hrq73J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
        "actual_labels = [1, 0, 1, 0, 1, 0, 0, 1, 1, 0] # Actual labels of the data\n",
        "predicted_labels = [1, 0, 1, 0, 0, 1, 1, 1, 0, 1] # Predicted labels from a model\n",
        "\n",
        "cm = confusion_matrix(actual_labels, predicted_labels) # Calculate confusion matrix\n",
        "precision = precision_score(actual_labels, predicted_labels)\n",
        "recall = recall_score(actual_labels, predicted_labels)\n",
        "f1 = f1_score(actual_labels, predicted_labels)\n",
        "accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy) #model correctly predicts the labels for 50% of the instances.\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm) #shows TP, FP, FN, and TN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSjoNgP6q_hM",
        "outputId": "8adfd56c-9e30-4bc6-e4ea-1c6a93ed9cc1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.5\n",
            "Recall: 0.6\n",
            "F1 Score: 0.5454545454545454\n",
            "Accuracy: 0.5\n",
            "Confusion Matrix:\n",
            "[[2 3]\n",
            " [2 3]]\n"
          ]
        }
      ]
    }
  ]
}